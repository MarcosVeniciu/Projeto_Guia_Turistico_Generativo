{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install googletrans==3.1.0a0\n",
    "! pip install langchain_community\n",
    "!pip install pdfplumber\n",
    "!pip install --upgrade cryptography pyOpenSSL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "def translate_text(text):\n",
    "    translator = Translator()\n",
    "    translation = translator.translate(text, dest='en')\n",
    "    return translation.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "\n",
    "def ler_pdf(file_path, view=False):\n",
    "    # Crie um objeto PDFPlumberLoader com o caminho do arquivo\n",
    "    loader = PDFPlumberLoader(file_path)\n",
    "\n",
    "    # Carregue o conteúdo do PDF\n",
    "    documents = loader.load()\n",
    "\n",
    "    if view:\n",
    "        # Exiba o conteúdo de cada página do documento\n",
    "        for idx, document in enumerate(documents, start=1):\n",
    "            print(f\"Conteúdo da página {idx}:\")\n",
    "            print(document.page_content)\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def limpar_documento(documents, view = False):\n",
    "    doc = []\n",
    "    # Exiba o conteúdo de cada página do documento\n",
    "    for idx, document in enumerate(documents, start=1):\n",
    "        if view:\n",
    "            print(f\"Conteúdo da página {idx}:\")\n",
    "        texto_completo = document.page_content\n",
    "\n",
    "        # Remover quebras de linha desnecessárias e substituir por espaços\n",
    "        texto_completo = re.sub(r'\\n+', ' ', texto_completo)\n",
    "        texto_completo = ' '.join(texto_completo.split())\n",
    "        texto_completo = translate_text(texto_completo)\n",
    "\n",
    "        # Separar o texto em linhas\n",
    "        linhas = texto_completo.split('--------------------------------------------------')\n",
    "        doc += linhas\n",
    "        if view:\n",
    "            # Limpar cada linha e exibir\n",
    "            for linha in linhas:\n",
    "                linha_limpa = re.sub(r'\\s+', ' ', linha.strip())\n",
    "                if linha_limpa:\n",
    "                    print(linha_limpa)\n",
    "                    print('-' * 50)\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "modelo_id = \"facebook/bart-large-cnn\"\n",
    "\n",
    "# Check if GPU is available\n",
    "device = -1 #0 if torch.cuda.is_available() else -1\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelo_id, max_new_tokens=100, device=device) #truncation=True\n",
    "summarizer = pipeline(\"summarization\", model=modelo_id, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumarizar_documento(doc, view=False):\n",
    "    resumos = []\n",
    "    for i in range(len(doc)):\n",
    "        if view:\n",
    "            print(f\"sumarizando pagina: ({i+1}/{len(doc)})\")\n",
    "        try:\n",
    "            if len(doc[i]) > 0:\n",
    "                doc[i] = ' '.join(doc[i].split())\n",
    "                num_token = len(tokenizer.tokenize(doc[i]))\n",
    "                resumo = summarizer(doc[i],\n",
    "                                    max_length=num_token,\n",
    "                                    min_length=int(num_token*0.6),\n",
    "                                    do_sample=True,\n",
    "                                    num_beams=6,\n",
    "                                    top_k=50,\n",
    "                                    temperature=0.1)\n",
    "                resumos += resumo\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar doc[{i}]: {e}\")\n",
    "            continue  # Ir para o próximo doc[i]\n",
    "        \n",
    "    if view:\n",
    "        for resumo in resumos:\n",
    "            print(\"\\n\")\n",
    "            print(resumo[\"summary_text\"])\n",
    "            print(\"\\n\\n\")\n",
    "    return resumos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def get_name(file_name):\n",
    "  return file_name[:len(file_name)-4] + \".txt\"\n",
    "\n",
    "\n",
    "def savar_resumo_txt(resumos, output_file):\n",
    "    # Nome do arquivo de saída  \n",
    "    output_file = get_name(output_file)\n",
    "    # Abra o arquivo de saída em modo de escrita\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        # Exiba o conteúdo de cada página do documento\n",
    "        for i in range(len(resumos)):\n",
    "            #file.write(f\"Conteúdo DDda página: \") # TENTA REMOVER ISSO PRA VER SE DA CERTO\n",
    "            texto_completo = resumos[i]['summary_text']\n",
    "\n",
    "            # Remover quebras de linha desnecessárias e substituir por espaços\n",
    "            texto_completo = re.sub(r'\\n+', ' ', texto_completo)\n",
    "            # Separar o texto em linhas\n",
    "            linhas = texto_completo.split('--------------------------------------------------')\n",
    "            # Limpar cada linha e escrever no arquivo\n",
    "            for linha in linhas:\n",
    "                linha_limpa = re.sub(r'\\s+', ' ', linha.strip())\n",
    "                if linha_limpa:\n",
    "                    file.write(linha_limpa + \"\\n\")\n",
    "                    #file.write('-' * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerar txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando PDFs:  71%|███████▏  | 92/129 [2:18:30<53:28, 86.70s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao processar doc[2]: Input length of decoder_input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando PDFs: 100%|██████████| 129/129 [3:12:56<00:00, 89.74s/it]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os \n",
    "\n",
    "local_pdf = \"PDF\"\n",
    "local_txt = \"TXT\"\n",
    "\n",
    "lista_pdf = os.listdir(local_pdf)\n",
    "\n",
    "for i in tqdm(range(len(lista_pdf)), desc=\"Processando PDFs\"):\n",
    "    pdf = lista_pdf[i]\n",
    "    file_path = os.path.join(local_pdf, pdf)\n",
    "    output_file = os.path.join(local_txt, pdf)\n",
    "    \n",
    "    documents = ler_pdf(file_path)\n",
    "    doc = limpar_documento(documents)\n",
    "    resumos = sumarizar_documento(doc)\n",
    "    savar_resumo_txt(resumos, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
