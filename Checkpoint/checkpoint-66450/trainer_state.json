{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 149.98478873239438,
  "eval_steps": 500,
  "global_step": 66450,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.9983098591549295,
      "grad_norm": 0.0,
      "learning_rate": 1.2490594431903688e-05,
      "loss": 3.25,
      "step": 443
    },
    {
      "epoch": 1.996619718309859,
      "grad_norm": 0.0,
      "learning_rate": 2.5793829947328817e-05,
      "loss": 3.2502,
      "step": 886
    },
    {
      "epoch": 2.994929577464789,
      "grad_norm": 0.0,
      "learning_rate": 3.912716328066216e-05,
      "loss": 3.2511,
      "step": 1329
    },
    {
      "epoch": 3.993239436619718,
      "grad_norm": 0.0,
      "learning_rate": 5.2460496613995494e-05,
      "loss": 3.2508,
      "step": 1772
    },
    {
      "epoch": 4.991549295774648,
      "grad_norm": 0.0,
      "learning_rate": 6.579382994732882e-05,
      "loss": 3.251,
      "step": 2215
    },
    {
      "epoch": 5.989859154929578,
      "grad_norm": 0.0,
      "learning_rate": 7.912716328066215e-05,
      "loss": 3.2514,
      "step": 2658
    },
    {
      "epoch": 6.988169014084507,
      "grad_norm": 0.0,
      "learning_rate": 9.243039879608729e-05,
      "loss": 3.2513,
      "step": 3101
    },
    {
      "epoch": 7.986478873239436,
      "grad_norm": 0.0,
      "learning_rate": 0.00010576373212942062,
      "loss": 3.2517,
      "step": 3544
    },
    {
      "epoch": 8.984788732394366,
      "grad_norm": 0.0,
      "learning_rate": 0.00011909706546275397,
      "loss": 3.2511,
      "step": 3987
    },
    {
      "epoch": 9.983098591549297,
      "grad_norm": 0.0,
      "learning_rate": 0.0001324303987960873,
      "loss": 3.2514,
      "step": 4430
    },
    {
      "epoch": 10.981408450704226,
      "grad_norm": 0.0,
      "learning_rate": 0.00014576373212942063,
      "loss": 3.2523,
      "step": 4873
    },
    {
      "epoch": 11.979718309859155,
      "grad_norm": 0.0,
      "learning_rate": 0.00015906696764484576,
      "loss": 3.2515,
      "step": 5316
    },
    {
      "epoch": 12.978028169014085,
      "grad_norm": 0.0,
      "learning_rate": 0.0001724003009781791,
      "loss": 3.2525,
      "step": 5759
    },
    {
      "epoch": 13.976338028169014,
      "grad_norm": 0.0,
      "learning_rate": 0.00018573363431151243,
      "loss": 3.2518,
      "step": 6202
    },
    {
      "epoch": 14.974647887323943,
      "grad_norm": 0.0,
      "learning_rate": 0.00019906696764484576,
      "loss": 3.2559,
      "step": 6645
    },
    {
      "epoch": 15.972957746478873,
      "grad_norm": 0.0,
      "learning_rate": 0.0002124003009781791,
      "loss": 3.2511,
      "step": 7088
    },
    {
      "epoch": 16.971267605633802,
      "grad_norm": 0.0,
      "learning_rate": 0.0002257035364936042,
      "loss": 3.2539,
      "step": 7531
    },
    {
      "epoch": 17.96957746478873,
      "grad_norm": 0.0,
      "learning_rate": 0.00023903686982693753,
      "loss": 3.254,
      "step": 7974
    },
    {
      "epoch": 18.96788732394366,
      "grad_norm": 0.0,
      "learning_rate": 0.0002523702031602709,
      "loss": 3.2526,
      "step": 8417
    },
    {
      "epoch": 19.966197183098593,
      "grad_norm": 0.0,
      "learning_rate": 0.00026570353649360426,
      "loss": 3.2556,
      "step": 8860
    },
    {
      "epoch": 20.964507042253523,
      "grad_norm": 0.0,
      "learning_rate": 0.00027903686982693756,
      "loss": 3.2521,
      "step": 9303
    },
    {
      "epoch": 21.99830985915493,
      "grad_norm": 0.0,
      "learning_rate": 0.00029152746425884125,
      "loss": 3.2499,
      "step": 9746
    },
    {
      "epoch": 22.99661971830986,
      "grad_norm": 0.0,
      "learning_rate": 0.0003048306997742664,
      "loss": 3.2505,
      "step": 10189
    },
    {
      "epoch": 23.994929577464788,
      "grad_norm": 0.0,
      "learning_rate": 0.00031816403310759974,
      "loss": 3.2505,
      "step": 10632
    },
    {
      "epoch": 24.993239436619717,
      "grad_norm": 0.0,
      "learning_rate": 0.00033149736644093303,
      "loss": 3.2511,
      "step": 11075
    },
    {
      "epoch": 25.991549295774647,
      "grad_norm": 0.0,
      "learning_rate": 0.0003448306997742664,
      "loss": 3.2512,
      "step": 11518
    },
    {
      "epoch": 26.989859154929576,
      "grad_norm": 0.0,
      "learning_rate": 0.00035816403310759973,
      "loss": 3.2515,
      "step": 11961
    },
    {
      "epoch": 27.98816901408451,
      "grad_norm": 0.0,
      "learning_rate": 0.00037146726862302486,
      "loss": 3.2513,
      "step": 12404
    },
    {
      "epoch": 28.986478873239438,
      "grad_norm": 0.0,
      "learning_rate": 0.00038480060195635816,
      "loss": 3.2511,
      "step": 12847
    },
    {
      "epoch": 29.984788732394367,
      "grad_norm": 0.0,
      "learning_rate": 0.0003981339352896915,
      "loss": 3.2516,
      "step": 13290
    },
    {
      "epoch": 30.983098591549297,
      "grad_norm": 0.0,
      "learning_rate": 0.00039985103225500157,
      "loss": 3.2518,
      "step": 13733
    },
    {
      "epoch": 31.981408450704226,
      "grad_norm": 0.0,
      "learning_rate": 0.0003993035364107689,
      "loss": 3.2513,
      "step": 14176
    },
    {
      "epoch": 32.97971830985915,
      "grad_norm": 0.0,
      "learning_rate": 0.0003983572623895707,
      "loss": 3.2532,
      "step": 14619
    },
    {
      "epoch": 33.97802816901408,
      "grad_norm": 0.0,
      "learning_rate": 0.000397009834798797,
      "loss": 3.2509,
      "step": 15062
    },
    {
      "epoch": 34.97633802816902,
      "grad_norm": 0.0,
      "learning_rate": 0.00039526565561932544,
      "loss": 3.2524,
      "step": 15505
    },
    {
      "epoch": 35.97464788732395,
      "grad_norm": 0.0,
      "learning_rate": 0.0003931282373959033,
      "loss": 3.2544,
      "step": 15948
    },
    {
      "epoch": 36.972957746478876,
      "grad_norm": 0.0,
      "learning_rate": 0.00039060188460437957,
      "loss": 3.2546,
      "step": 16391
    },
    {
      "epoch": 37.971267605633805,
      "grad_norm": 0.0,
      "learning_rate": 0.0003876986822891734,
      "loss": 3.2528,
      "step": 16834
    },
    {
      "epoch": 38.969577464788735,
      "grad_norm": 0.0,
      "learning_rate": 0.00038441134266136725,
      "loss": 3.2522,
      "step": 17277
    },
    {
      "epoch": 39.967887323943664,
      "grad_norm": 0.0,
      "learning_rate": 0.0003807526231313874,
      "loss": 3.2534,
      "step": 17720
    },
    {
      "epoch": 40.96619718309859,
      "grad_norm": 0.0,
      "learning_rate": 0.00037672989187345526,
      "loss": 3.253,
      "step": 18163
    },
    {
      "epoch": 41.96450704225352,
      "grad_norm": 0.0,
      "learning_rate": 0.00037235125013296874,
      "loss": 3.2533,
      "step": 18606
    },
    {
      "epoch": 42.99830985915493,
      "grad_norm": 0.0,
      "learning_rate": 0.00036793430527108076,
      "loss": 3.25,
      "step": 19049
    },
    {
      "epoch": 43.99661971830986,
      "grad_norm": 0.0,
      "learning_rate": 0.0003629037927762376,
      "loss": 3.2502,
      "step": 19492
    },
    {
      "epoch": 44.99492957746479,
      "grad_norm": 0.0,
      "learning_rate": 0.00035753421050839686,
      "loss": 3.2507,
      "step": 19935
    },
    {
      "epoch": 45.99323943661972,
      "grad_norm": 0.0,
      "learning_rate": 0.0003518473753056592,
      "loss": 3.251,
      "step": 20378
    },
    {
      "epoch": 46.99154929577465,
      "grad_norm": 0.0,
      "learning_rate": 0.0003458547396973038,
      "loss": 3.2509,
      "step": 20821
    },
    {
      "epoch": 47.989859154929576,
      "grad_norm": 0.0,
      "learning_rate": 0.0003395683720539303,
      "loss": 3.2519,
      "step": 21264
    },
    {
      "epoch": 48.988169014084505,
      "grad_norm": 0.0,
      "learning_rate": 0.00033301606391015955,
      "loss": 3.2506,
      "step": 21707
    },
    {
      "epoch": 49.986478873239435,
      "grad_norm": 0.0,
      "learning_rate": 0.00032618136727735977,
      "loss": 3.2519,
      "step": 22150
    },
    {
      "epoch": 50.984788732394364,
      "grad_norm": 0.0,
      "learning_rate": 0.00031909255816341153,
      "loss": 3.2516,
      "step": 22593
    },
    {
      "epoch": 51.98309859154929,
      "grad_norm": 0.0,
      "learning_rate": 0.000311763912486473,
      "loss": 3.252,
      "step": 23036
    },
    {
      "epoch": 52.98140845070422,
      "grad_norm": 0.0,
      "learning_rate": 0.000304210189163622,
      "loss": 3.2513,
      "step": 23479
    },
    {
      "epoch": 53.97971830985915,
      "grad_norm": 0.0,
      "learning_rate": 0.0002964643501045626,
      "loss": 3.252,
      "step": 23922
    },
    {
      "epoch": 54.97802816901408,
      "grad_norm": 0.0,
      "learning_rate": 0.0002885069512492795,
      "loss": 3.2513,
      "step": 24365
    },
    {
      "epoch": 55.97633802816902,
      "grad_norm": 0.0,
      "learning_rate": 0.0002803713111729181,
      "loss": 3.2527,
      "step": 24808
    },
    {
      "epoch": 56.97464788732395,
      "grad_norm": 0.0,
      "learning_rate": 0.0002720738139718799,
      "loss": 3.2535,
      "step": 25251
    },
    {
      "epoch": 57.972957746478876,
      "grad_norm": 0.0,
      "learning_rate": 0.000263631169701275,
      "loss": 3.2561,
      "step": 25694
    },
    {
      "epoch": 58.971267605633805,
      "grad_norm": 0.0,
      "learning_rate": 0.0002550798592839168,
      "loss": 3.2522,
      "step": 26137
    },
    {
      "epoch": 59.969577464788735,
      "grad_norm": 0.0,
      "learning_rate": 0.000246398416718292,
      "loss": 3.251,
      "step": 26580
    },
    {
      "epoch": 60.967887323943664,
      "grad_norm": 0.0,
      "learning_rate": 0.00023762353391621973,
      "loss": 3.2534,
      "step": 27023
    },
    {
      "epoch": 61.96619718309859,
      "grad_norm": 0.0,
      "learning_rate": 0.00022877288232387905,
      "loss": 3.2551,
      "step": 27466
    },
    {
      "epoch": 62.96450704225352,
      "grad_norm": 0.0,
      "learning_rate": 0.00021986428597570891,
      "loss": 3.2552,
      "step": 27909
    },
    {
      "epoch": 63.99830985915493,
      "grad_norm": 0.0,
      "learning_rate": 0.00021148212581059434,
      "loss": 3.2498,
      "step": 28352
    },
    {
      "epoch": 64.99661971830986,
      "grad_norm": 0.0,
      "learning_rate": 0.00020253265776044797,
      "loss": 3.2503,
      "step": 28795
    },
    {
      "epoch": 65.99492957746479,
      "grad_norm": 0.0,
      "learning_rate": 0.0001935578609487608,
      "loss": 3.2508,
      "step": 29238
    },
    {
      "epoch": 66.99323943661972,
      "grad_norm": 0.0,
      "learning_rate": 0.00018459603774778591,
      "loss": 3.2512,
      "step": 29681
    },
    {
      "epoch": 67.99154929577465,
      "grad_norm": 0.0,
      "learning_rate": 0.00017566523607678728,
      "loss": 3.251,
      "step": 30124
    },
    {
      "epoch": 68.98985915492958,
      "grad_norm": 0.0,
      "learning_rate": 0.00016678344138179555,
      "loss": 3.2513,
      "step": 30567
    },
    {
      "epoch": 69.9881690140845,
      "grad_norm": 0.0,
      "learning_rate": 0.00015798834993963778,
      "loss": 3.2514,
      "step": 31010
    },
    {
      "epoch": 70.98647887323943,
      "grad_norm": 0.0,
      "learning_rate": 0.00014925788379535802,
      "loss": 3.2517,
      "step": 31453
    },
    {
      "epoch": 71.98478873239436,
      "grad_norm": 0.0,
      "learning_rate": 0.00014062960552022014,
      "loss": 3.2507,
      "step": 31896
    },
    {
      "epoch": 72.9830985915493,
      "grad_norm": 0.0,
      "learning_rate": 0.00013212089131839634,
      "loss": 3.2524,
      "step": 32339
    },
    {
      "epoch": 73.98140845070422,
      "grad_norm": 0.0,
      "learning_rate": 0.00012374887660792703,
      "loss": 3.2512,
      "step": 32782
    },
    {
      "epoch": 74.97971830985915,
      "grad_norm": 0.0,
      "learning_rate": 0.00011554878792964694,
      "loss": 3.2524,
      "step": 33225
    },
    {
      "epoch": 75.97802816901408,
      "grad_norm": 0.0,
      "learning_rate": 0.00010750004093947472,
      "loss": 3.2527,
      "step": 33668
    },
    {
      "epoch": 76.97633802816901,
      "grad_norm": 0.0,
      "learning_rate": 9.963757655671248e-05,
      "loss": 3.2509,
      "step": 34111
    },
    {
      "epoch": 77.97464788732394,
      "grad_norm": 0.0,
      "learning_rate": 9.197722873826999e-05,
      "loss": 3.2542,
      "step": 34554
    },
    {
      "epoch": 78.97295774647887,
      "grad_norm": 0.0,
      "learning_rate": 8.453442440519404e-05,
      "loss": 3.2545,
      "step": 34997
    },
    {
      "epoch": 79.9712676056338,
      "grad_norm": 0.0,
      "learning_rate": 7.73401555443747e-05,
      "loss": 3.2533,
      "step": 35440
    },
    {
      "epoch": 80.96957746478873,
      "grad_norm": 0.0,
      "learning_rate": 7.03763626796945e-05,
      "loss": 3.2538,
      "step": 35883
    },
    {
      "epoch": 81.96788732394366,
      "grad_norm": 0.0,
      "learning_rate": 6.367361456992289e-05,
      "loss": 3.2527,
      "step": 36326
    },
    {
      "epoch": 82.96619718309859,
      "grad_norm": 0.0,
      "learning_rate": 5.7245409657733664e-05,
      "loss": 3.2527,
      "step": 36769
    },
    {
      "epoch": 83.96450704225352,
      "grad_norm": 0.0,
      "learning_rate": 5.1104693492337555e-05,
      "loss": 3.2548,
      "step": 37212
    },
    {
      "epoch": 84.96281690140844,
      "grad_norm": 0.0,
      "learning_rate": 4.527667083606348e-05,
      "loss": 3.2517,
      "step": 37655
    },
    {
      "epoch": 85.96112676056337,
      "grad_norm": 0.0,
      "learning_rate": 3.974671184423806e-05,
      "loss": 3.2538,
      "step": 38098
    },
    {
      "epoch": 86.9594366197183,
      "grad_norm": 0.0,
      "learning_rate": 3.453948164782941e-05,
      "loss": 3.2532,
      "step": 38541
    },
    {
      "epoch": 87.99830985915493,
      "grad_norm": 0.0,
      "learning_rate": 0.00025811792812461344,
      "loss": 3.2499,
      "step": 38984
    },
    {
      "epoch": 88.99661971830986,
      "grad_norm": 0.0,
      "learning_rate": 0.00025238004347533186,
      "loss": 3.2506,
      "step": 39427
    },
    {
      "epoch": 89.99492957746479,
      "grad_norm": 0.0,
      "learning_rate": 0.00024658234675126075,
      "loss": 3.2503,
      "step": 39870
    },
    {
      "epoch": 90.99323943661972,
      "grad_norm": 0.0,
      "learning_rate": 0.00024074295251806058,
      "loss": 3.2514,
      "step": 40313
    },
    {
      "epoch": 91.99154929577465,
      "grad_norm": 0.0,
      "learning_rate": 0.0002348670878244167,
      "loss": 3.2509,
      "step": 40756
    },
    {
      "epoch": 92.98985915492958,
      "grad_norm": 0.0,
      "learning_rate": 0.00022896001236501597,
      "loss": 3.2514,
      "step": 41199
    },
    {
      "epoch": 93.9881690140845,
      "grad_norm": 0.0,
      "learning_rate": 0.00022304043175955354,
      "loss": 3.2512,
      "step": 41642
    },
    {
      "epoch": 94.98647887323943,
      "grad_norm": 0.0,
      "learning_rate": 0.00021708686140451232,
      "loss": 3.2513,
      "step": 42085
    },
    {
      "epoch": 95.98478873239436,
      "grad_norm": 0.0,
      "learning_rate": 0.0002111179959943893,
      "loss": 3.2521,
      "step": 42528
    },
    {
      "epoch": 96.9830985915493,
      "grad_norm": 0.0,
      "learning_rate": 0.00020513917847210782,
      "loss": 3.2518,
      "step": 42971
    },
    {
      "epoch": 97.98140845070422,
      "grad_norm": 0.0,
      "learning_rate": 0.00019915576068907945,
      "loss": 3.2513,
      "step": 43414
    },
    {
      "epoch": 98.97971830985915,
      "grad_norm": 0.0,
      "learning_rate": 0.00019318659862748371,
      "loss": 3.2524,
      "step": 43857
    },
    {
      "epoch": 99.97802816901408,
      "grad_norm": 0.0,
      "learning_rate": 0.00018721002773221606,
      "loss": 3.2546,
      "step": 44300
    },
    {
      "epoch": 100.97633802816901,
      "grad_norm": 0.0,
      "learning_rate": 0.00018124490559431235,
      "loss": 3.2512,
      "step": 44743
    },
    {
      "epoch": 101.97464788732394,
      "grad_norm": 0.0,
      "learning_rate": 0.00017529657180596022,
      "loss": 3.2519,
      "step": 45186
    },
    {
      "epoch": 102.97295774647887,
      "grad_norm": 0.0,
      "learning_rate": 0.00016937035093150108,
      "loss": 3.2551,
      "step": 45629
    },
    {
      "epoch": 103.9712676056338,
      "grad_norm": 0.0,
      "learning_rate": 0.00016348482848364696,
      "loss": 3.2528,
      "step": 46072
    },
    {
      "epoch": 104.96957746478873,
      "grad_norm": 0.0,
      "learning_rate": 0.0001576186434700048,
      "loss": 3.2523,
      "step": 46515
    },
    {
      "epoch": 105.96788732394366,
      "grad_norm": 0.0,
      "learning_rate": 0.00015179039551029106,
      "loss": 3.253,
      "step": 46958
    },
    {
      "epoch": 106.96619718309859,
      "grad_norm": 0.0,
      "learning_rate": 0.00014600530167576664,
      "loss": 3.2547,
      "step": 47401
    },
    {
      "epoch": 107.96450704225352,
      "grad_norm": 0.0,
      "learning_rate": 0.00014026854040890592,
      "loss": 3.254,
      "step": 47844
    },
    {
      "epoch": 108.96281690140844,
      "grad_norm": 0.0,
      "learning_rate": 0.0001345980119573994,
      "loss": 3.2531,
      "step": 48287
    },
    {
      "epoch": 109.96112676056337,
      "grad_norm": 0.0,
      "learning_rate": 0.00012897313563088505,
      "loss": 3.2514,
      "step": 48730
    },
    {
      "epoch": 110.9594366197183,
      "grad_norm": 0.0,
      "learning_rate": 0.00012341183796736182,
      "loss": 3.2546,
      "step": 49173
    },
    {
      "epoch": 111.95774647887323,
      "grad_norm": 0.0,
      "learning_rate": 0.00011791909708142193,
      "loss": 3.2532,
      "step": 49616
    },
    {
      "epoch": 112.95605633802816,
      "grad_norm": 0.0,
      "learning_rate": 0.00011249982972005648,
      "loss": 3.2528,
      "step": 50059
    },
    {
      "epoch": 113.9543661971831,
      "grad_norm": 0.0,
      "learning_rate": 0.00010717085136427458,
      "loss": 3.254,
      "step": 50502
    },
    {
      "epoch": 114.95267605633803,
      "grad_norm": 0.0,
      "learning_rate": 0.00010191282095011317,
      "loss": 3.2525,
      "step": 50945
    },
    {
      "epoch": 115.95098591549296,
      "grad_norm": 0.0,
      "learning_rate": 9.674259184544165e-05,
      "loss": 3.254,
      "step": 51388
    },
    {
      "epoch": 116.9492957746479,
      "grad_norm": 0.0,
      "learning_rate": 9.166479210552831e-05,
      "loss": 3.2558,
      "step": 51831
    },
    {
      "epoch": 117.94760563380282,
      "grad_norm": 0.0,
      "learning_rate": 8.668396704884196e-05,
      "loss": 3.2522,
      "step": 52274
    },
    {
      "epoch": 118.94591549295775,
      "grad_norm": 0.0,
      "learning_rate": 8.181547209811229e-05,
      "loss": 3.2532,
      "step": 52717
    },
    {
      "epoch": 119.94422535211268,
      "grad_norm": 0.0,
      "learning_rate": 7.704163747388811e-05,
      "loss": 3.2539,
      "step": 53160
    },
    {
      "epoch": 120.94253521126761,
      "grad_norm": 0.0,
      "learning_rate": 7.237786723674382e-05,
      "loss": 3.2535,
      "step": 53603
    },
    {
      "epoch": 121.94084507042254,
      "grad_norm": 0.0,
      "learning_rate": 6.782833609267016e-05,
      "loss": 3.2539,
      "step": 54046
    },
    {
      "epoch": 122.93915492957747,
      "grad_norm": 0.0,
      "learning_rate": 6.339711648819764e-05,
      "loss": 3.2532,
      "step": 54489
    },
    {
      "epoch": 123.9374647887324,
      "grad_norm": 0.0,
      "learning_rate": 5.909776108207932e-05,
      "loss": 3.2533,
      "step": 54932
    },
    {
      "epoch": 124.93577464788733,
      "grad_norm": 0.0,
      "learning_rate": 5.4914665737394834e-05,
      "loss": 3.2546,
      "step": 55375
    },
    {
      "epoch": 125.93408450704226,
      "grad_norm": 0.0,
      "learning_rate": 5.086144141621647e-05,
      "loss": 3.2548,
      "step": 55818
    },
    {
      "epoch": 126.93239436619719,
      "grad_norm": 0.0,
      "learning_rate": 4.6941716303251946e-05,
      "loss": 3.2537,
      "step": 56261
    },
    {
      "epoch": 127.93070422535212,
      "grad_norm": 0.0,
      "learning_rate": 4.315899908333758e-05,
      "loss": 3.2521,
      "step": 56704
    },
    {
      "epoch": 128.92901408450703,
      "grad_norm": 0.0,
      "learning_rate": 3.952473718003546e-05,
      "loss": 3.2551,
      "step": 57147
    },
    {
      "epoch": 129.92732394366197,
      "grad_norm": 0.0,
      "learning_rate": 3.6025740355510495e-05,
      "loss": 3.2513,
      "step": 57590
    },
    {
      "epoch": 130.9256338028169,
      "grad_norm": 0.0,
      "learning_rate": 3.267352270089219e-05,
      "loss": 3.257,
      "step": 58033
    },
    {
      "epoch": 131.92394366197183,
      "grad_norm": 0.0,
      "learning_rate": 2.9471084904982272e-05,
      "loss": 3.2513,
      "step": 58476
    },
    {
      "epoch": 132.92225352112675,
      "grad_norm": 0.0,
      "learning_rate": 2.642129358332375e-05,
      "loss": 3.2545,
      "step": 58919
    },
    {
      "epoch": 133.9205633802817,
      "grad_norm": 0.0,
      "learning_rate": 2.3533235438174673e-05,
      "loss": 3.2558,
      "step": 59362
    },
    {
      "epoch": 134.9188732394366,
      "grad_norm": 0.0,
      "learning_rate": 2.0796428513410325e-05,
      "loss": 3.2523,
      "step": 59805
    },
    {
      "epoch": 135.91718309859155,
      "grad_norm": 0.0,
      "learning_rate": 1.8220033055202545e-05,
      "loss": 3.2557,
      "step": 60248
    },
    {
      "epoch": 136.91549295774647,
      "grad_norm": 0.0,
      "learning_rate": 1.5806355286415408e-05,
      "loss": 3.2541,
      "step": 60691
    },
    {
      "epoch": 137.9138028169014,
      "grad_norm": 0.0,
      "learning_rate": 1.3557555775543584e-05,
      "loss": 3.2555,
      "step": 61134
    },
    {
      "epoch": 138.91211267605632,
      "grad_norm": 0.0,
      "learning_rate": 1.1480157704524131e-05,
      "loss": 3.2505,
      "step": 61577
    },
    {
      "epoch": 139.91042253521127,
      "grad_norm": 0.0,
      "learning_rate": 9.56662133754167e-06,
      "loss": 3.256,
      "step": 62020
    },
    {
      "epoch": 140.9087323943662,
      "grad_norm": 0.0,
      "learning_rate": 7.823548635424583e-06,
      "loss": 3.2518,
      "step": 62463
    },
    {
      "epoch": 141.99830985915494,
      "grad_norm": 0.0,
      "learning_rate": 6.346678200295508e-06,
      "loss": 3.25,
      "step": 62906
    },
    {
      "epoch": 142.99661971830986,
      "grad_norm": 0.0,
      "learning_rate": 4.941042840758203e-06,
      "loss": 3.2503,
      "step": 63349
    },
    {
      "epoch": 143.9949295774648,
      "grad_norm": 0.0,
      "learning_rate": 3.7066353432587463e-06,
      "loss": 3.2507,
      "step": 63792
    },
    {
      "epoch": 144.99323943661972,
      "grad_norm": 0.0,
      "learning_rate": 2.6479369917250087e-06,
      "loss": 3.2513,
      "step": 64235
    },
    {
      "epoch": 145.99154929577466,
      "grad_norm": 0.0,
      "learning_rate": 1.7658954645642933e-06,
      "loss": 3.2506,
      "step": 64678
    },
    {
      "epoch": 146.98985915492958,
      "grad_norm": 0.0,
      "learning_rate": 1.0613003084009477e-06,
      "loss": 3.252,
      "step": 65121
    },
    {
      "epoch": 147.98816901408452,
      "grad_norm": 0.0,
      "learning_rate": 5.357698400506372e-07,
      "loss": 3.2511,
      "step": 65564
    },
    {
      "epoch": 148.98647887323943,
      "grad_norm": 0.0,
      "learning_rate": 1.8739669233629465e-07,
      "loss": 3.2515,
      "step": 66007
    },
    {
      "epoch": 149.98478873239438,
      "grad_norm": 0.0,
      "learning_rate": 1.7882885801268067e-08,
      "loss": 3.2515,
      "step": 66450
    }
  ],
  "logging_steps": 443,
  "max_steps": 66450,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 150,
  "save_steps": 1329,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.709180670642266e+18,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
